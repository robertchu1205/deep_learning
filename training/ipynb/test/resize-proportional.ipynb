{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParam():\n",
    "    def __init__(self):\n",
    "        self.channels = 3\n",
    "        self.image_max_size = 100\n",
    "        self.image_size = 32\n",
    "        self.input_shape = (self.image_size, self.image_size, self.channels)\n",
    "        \n",
    "        self.shuffle_buffer = 10000\n",
    "        self.batch_size = 1024\n",
    "        self.valid_size = 3000\n",
    "        \n",
    "        self.epochs = 100\n",
    "        self.steps_per_epoch = 10\n",
    "\n",
    "hparam = HParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "There are 3 methods to resize a image in tensorflow:\n",
    "1. resize: arbitrary resize image to given size, do not preserve aspect ratio\n",
    "2. resize_with_pad: preserve aspect ratio, resize image to given size, do not scale proportionally\n",
    "3. resize_with_crop_or_pad: do not resize image, simply central crop or margin pad to given size\n",
    "\n",
    "To resize image proportionally, the idea is that we first pad all images to a bigger size, and then resize them down to a smaller desiered size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DEGREE = 4\n",
    "N_LABEL = 6\n",
    "\n",
    "def parse_image(image):\n",
    "    image = tf.io.decode_png(image, hparam.channels)\n",
    "    image = tf.image.resize_with_pad(image, hparam.image_size, hparam.image_size)\n",
    "    return image\n",
    "\n",
    "def parse_image_proportional(image):\n",
    "    image = tf.io.decode_png(image, hparam.channels)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, hparam.image_max_size, hparam.image_max_size)\n",
    "    image = tf.image.resize(image, (hparam.image_size, hparam.image_size))\n",
    "    return image\n",
    "\n",
    "def parse_single_example(example_proto, img_parse_fn):\n",
    "    feature_spec = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"degree\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "    features[\"image\"] = img_parse_fn(features[\"image\"])\n",
    "    features[\"degree\"] = tf.one_hot(features[\"degree\"], N_DEGREE)\n",
    "    features[\"label\"] = tf.one_hot(features[\"label\"], N_LABEL)\n",
    "\n",
    "    label = features[\"label\"]\n",
    "    return features, label\n",
    "\n",
    "tfrecord_paths = [\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/NG-InversePolarity.tfrecord',\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/NG-MoreComp.tfrecord',\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/NG-NoneComp.tfrecord',\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/NG-OutsidePosition.tfrecord',\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/NG-UpsideDown.tfrecord',\n",
    "    '/data/aoi-wzs-p3-dip-prewave-saiap/experiments/adversarial-training/OK.tfrecord',\n",
    "]\n",
    "datasets = [tf.data.TFRecordDataset(x).repeat() for x in tfrecord_paths]\n",
    "dataset = tf.data.experimental.sample_from_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot parsed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img_parse_fn):\n",
    "    n_col = 5\n",
    "    for x, _ in dataset.map(lambda x: parse_single_example(x, img_parse_fn), tf.data.experimental.AUTOTUNE).batch(n_col**2).take(1):\n",
    "        pass\n",
    "    x[\"image\"] /= 255.\n",
    "\n",
    "    fig, axes = plt.subplots(n_col, n_col, sharex=True, sharey=True, figsize=(10, 10))\n",
    "    for ax, im in zip(axes.flatten(), x[\"image\"]):\n",
    "        ax.imshow(im)\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(parse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(parse_image_proportional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as l\n",
    "\n",
    "def BaseModel():\n",
    "    inputs = {\n",
    "        \"image\": tf.keras.Input(hparam.input_shape, name=\"image\"),\n",
    "        \"degree\": tf.keras.Input([N_DEGREE], name=\"degree\")\n",
    "    }\n",
    "    e = inputs[\"degree\"]\n",
    "    \n",
    "    x = inputs[\"image\"]\n",
    "    x = l.Conv2D(64, 7, padding=\"same\")(x)\n",
    "    x = l.ReLU()(x)\n",
    "    for filters in [128, 256, 512]:\n",
    "        x = l.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = l.BatchNormalization()(x)\n",
    "        x = l.ReLU()(x)\n",
    "        x = l.MaxPool2D()(x)\n",
    "    x = l.Conv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = l.BatchNormalization()(x)\n",
    "    x = l.ReLU()(x)\n",
    "    x = l.GlobalAveragePooling2D()(x)\n",
    "    x = l.Concatenate()([x, e])\n",
    "    x = l.Dense(N_LABEL)(x)\n",
    "    output = l.Activation(\"softmax\", dtype=\"float32\")(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.Recall(name=\"recall/NG-InversePolarity\", class_id=0),\n",
    "    tf.keras.metrics.Recall(name=\"recall/NG-MoreComp\", class_id=1),\n",
    "    tf.keras.metrics.Recall(name=\"recall/NG-NoneComp\", class_id=2),\n",
    "    tf.keras.metrics.Recall(name=\"recall/NG-OutsidePosition\", class_id=3),\n",
    "    tf.keras.metrics.Recall(name=\"recall/NG-UpsideDown\", class_id=4),\n",
    "    tf.keras.metrics.Recall(name=\"recall/OK\", class_id=5),\n",
    "    tf.keras.metrics.Precision(name=\"precision/NG-InversePolarity\", class_id=0),\n",
    "    tf.keras.metrics.Precision(name=\"precision/NG-MoreComp\", class_id=1),\n",
    "    tf.keras.metrics.Precision(name=\"precision/NG-NoneComp\", class_id=2),\n",
    "    tf.keras.metrics.Precision(name=\"precision/NG-OutsidePosition\", class_id=3),\n",
    "    tf.keras.metrics.Precision(name=\"precision/NG-UpsideDown\", class_id=4),\n",
    "    tf.keras.metrics.Precision(name=\"precision/OK\", class_id=5),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.skip(hparam.valid_size).shuffle(hparam.shuffle_buffer).map(lambda x: parse_single_example(x, parse_image), tf.data.experimental.AUTOTUNE).batch(hparam.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = dataset.take(hparam.valid_size).shuffle(hparam.shuffle_buffer).map(lambda x: parse_single_example(x, parse_image), tf.data.experimental.AUTOTUNE).batch(hparam.batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/data/aoi-wzs-p3-dip-prewave-saiap/experiments/resize-proportional/parse_image\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(logdir, write_graph=False, profile_batch=0)\n",
    "]\n",
    "\n",
    "model = BaseModel()\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=metrics)\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds,\n",
    "          epochs=hparam.epochs, steps_per_epoch=hparam.steps_per_epoch,\n",
    "          callbacks=callbacks,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse_image_proportional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.skip(hparam.valid_size).shuffle(hparam.shuffle_buffer).map(lambda x: parse_single_example(x, parse_image_proportional), tf.data.experimental.AUTOTUNE).batch(hparam.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = dataset.take(hparam.valid_size).shuffle(hparam.shuffle_buffer).map(lambda x: parse_single_example(x, parse_image_proportional), tf.data.experimental.AUTOTUNE).batch(hparam.batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/data/aoi-wzs-p3-dip-prewave-saiap/experiments/resize-proportional/parse_image_proportional\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(logdir, write_graph=False, profile_batch=0)\n",
    "]\n",
    "\n",
    "model = BaseModel()\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=metrics)\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds,\n",
    "          epochs=hparam.epochs, steps_per_epoch=hparam.steps_per_epoch,\n",
    "          callbacks=callbacks,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
